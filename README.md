# Adversarial Attacks on CNNs: FGSM, BIM, and PGD

## Introduction

Adversarial attacks are a significant challenge in the field of machine learning, where small, carefully crafted perturbations to input data can lead models to make incorrect predictions with high confidence. These perturbations are often imperceptible to humans but can cause even well-trained models to fail. Understanding and defending against these attacks is crucial for deploying robust machine learning systems, especially in security-sensitive applications.

## Adversarial Attack Methods

### Fast Gradient Sign Method (FGSM)
The Fast Gradient Sign Method (FGSM) is one of the simplest and most widely known adversarial attack techniques. It generates adversarial examples by adjusting each pixel of an input image in the direction of the gradient of the loss with respect to the input, scaled by a factor called epsilon (ε). This one-step method is computationally efficient and serves as a baseline for evaluating model robustness.

**Reference**: Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572). *International Conference on Learning Representations (ICLR)*.

### Basic Iterative Method (BIM)
The Basic Iterative Method (BIM) builds on FGSM by applying the perturbation iteratively over several steps. At each step, the perturbation is recalculated and applied in smaller increments (controlled by a step size α). This iterative process allows for more precise and effective adversarial examples, making BIM a more potent attack than FGSM.

**Reference**: Kurakin, A., Goodfellow, I., & Bengio, S. (2017). [Adversarial Machine Learning at Scale](https://arxiv.org/abs/1611.01236). *International Conference on Learning Representations (ICLR)*.

### Projected Gradient Descent (PGD)
Projected Gradient Descent (PGD) is considered one of the most powerful adversarial attack methods. It extends BIM by introducing a random initialization of the input within the allowed perturbation range. PGD then iteratively applies the perturbation like BIM but ensures that after each step, the perturbed input is projected back into a valid space, maintaining the constraint imposed by epsilon. This makes PGD highly effective, particularly against models that have been adversarially trained.

**Reference**: Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2018). [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083). *International Conference on Learning Representations (ICLR)*.

## Project Summary

In this project, we implemented and experimented with the above three adversarial attack methods on a simple Convolutional Neural Network (CNN) trained on the MNIST dataset. The steps include:

1. **CNN Model Definition**: We defined a basic CNN architecture consisting of two convolutional layers, followed by fully connected layers, designed to classify handwritten digits from the MNIST dataset.

2. **Adversarial Attack Implementation**: We implemented the FGSM, BIM, and PGD attacks to generate adversarial examples. These attacks were tested on the CNN to evaluate how easily the model could be fooled by small perturbations in the input images.

3. **Adversarial Training**: To improve the model's robustness, we trained the CNN using a combination of clean and adversarial examples generated by FGSM. This method helps the model learn to classify adversarially perturbed inputs correctly.

4. **Robustness Assessment**: After training, we assessed the model's robustness against FGSM, BIM, and PGD attacks by measuring the success rate of each attack at various levels of perturbation (ε values).

5. **Visual Comparison**: We also visually compared the adversarial examples generated by FGSM, BIM, and PGD to understand the differences in their impact on the model's predictions.

## Conclusion

The experiments demonstrated that while adversarial training with FGSM can significantly enhance the model's resilience to simple attacks, more sophisticated methods like BIM and PGD still pose a substantial threat, especially at higher levels of perturbation. This highlights the need for ongoing research into more robust defense mechanisms to counter adversarial attacks effectively.

